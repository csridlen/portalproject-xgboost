confusion1= table(y = hotelsdev_test$children,
yhat = yhat)
confusion1
predicted1 <- predict(logit_hotelsdev, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 >0.5, 1, 0)
confusion1= table(y = hotelsdev_test$children,
yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)
predicted1 <- predict(logit_hotelsdev, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 > .49,1,0)
confusion1= table(y = hotelsdev_test$children,
yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)
predicted1 <- predict(logit_hotelsdev, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 > .4,1,0)
confusion1= table(y = hotelsdev_test$children,
yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)
summary(predicted1)
logit_hotelsdev = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotels_dev, family ='binomial')
hotelsdev_split = initial_split(hotels_dev, prop = 0.8)
hotelsdev_train = training(hotelsdev_split)
hotelsdev_test = testing(hotelsdev_split)
logit_hotelsdev = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotels_dev_train, family = 'binomial')
predicted1 <- predict(logit_hotelsdev, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 > .4,1,0)
confusion1= table(y = hotelsdev_test$children,
yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)
hotelsdev_split = initial_split(hotels_dev, prop = 0.8)
hotelsdev_train = training(hotelsdev_split)
hotelsdev_test = testing(hotelsdev_split)
logit_hotelsdev = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotelsdev_train, family = 'binomial')
predicted1 <- predict(logit_hotelsdev, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 > .4,1,0)
confusion1= table(y = hotelsdev_test$children,
yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)
summary(predicted1)
logit_hotelsdev = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotelsdev_train, family = 'binomial')
coef(logit_hotelsdev)
predicted1 <- predict(logit_hotelsdev, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 > .5, 1, 0)
confusion1= table(y = hotelsdev_test$children,
yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)
logit_hotel1 <- glm(children ~ . - arrival_date, data = hotels_dev, family='binomial')
rmse(logit_hotel1, data = hotelsdev_train)
predicted2 <- predict(logit_hotel1, hotelsdev_test, type = 'response')
yhat2<- ifelse(predicted2 > 0.5, 1, 0)
confusion2 = table(y = hotelsdev_test$children,
yhat = yhat2
logit_hotel1 <- glm(children ~ . - arrival_date, data = hotels_dev, family='binomial')
rmse(logit_hotel1, data = hotelsdev_train)
predicted2 <- predict(logit_hotel1, hotelsdev_test, type = 'response')
yhat2<- ifelse(predicted2 > 0.5, 1, 0)
confusion2 = table(y = hotelsdev_test$children,
yhat = yhat2)
logit_hotel1 <- glm(children ~ . - arrival_date, data = hotels_dev, family='binomial')
predicted2 <- predict(logit_hotel1, hotelsdev_test, type = 'response')
yhat2<- ifelse(predicted2 > 0.5, 1, 0)
confusion2 = table(y = hotelsdev_test$children,
yhat = yhat2)
confusion2
confusion2
sum(diag(confusion2))/sum(confusion2)
lm_best= glm(children ~ . - arrival_date - days_in_waiting_list - required_car_parking_spaces - previous_bookings_not_canceled + is_summer:stays_in_week_nights + is_holiday:stays_in_weekend_nights + is_repeated_guest:average_daily_rate, family = 'binomial', data = hotelsdev_train)
predicted <- predict(lm_best, hotelsdev_test, type = 'response')
yhat <- ifelse(predicted > 0.5, 1, 0)
confusion = table(y = hotelsdev_test$children,
yhat = yhat)
confusion
sum(diag(confusion))/sum(confusion)
lm_best= glm(children ~ . - arrival_date - days_in_waiting_list - required_car_parking_spaces - previous_bookings_not_canceled+is_repeated_guest:average_daily_rate, family = 'binomial', data = hotelsdev_train)
predicted <- predict(lm_best, hotelsdev_test, type = 'response')
yhat <- ifelse(predicted > 0.5, 1, 0)
confusion = table(y = hotelsdev_test$children,
yhat = yhat)
confusion
sum(diag(confusion))/sum(confusion)
lm_best= glm(children ~ . - arrival_date - days_in_waiting_list - required_car_parking_spaces - previous_bookings_not_canceled+is_repeated_guest:average_daily_rate + is_holiday*average_daily_rate + is_summer*stays_in_week_nights + is_holiday*stays_in_weekend_nights,family = 'binomial', data = hotelsdev_train)
predicted <- predict(lm_best, hotelsdev_test, type = 'response')
yhat <- ifelse(predicted > 0.5, 1, 0)
confusion = table(y = hotelsdev_test$children,
yhat = yhat)
confusion
sum(diag(confusion))/sum(confusion)
?labs
?kable
summ <- data.frame(c(1, 2, 3, 4), c(2, 3, 4, 5,), c(2, 3, 4, 5,))
summ <- data.frame(c(1, 2, 3, 4), c(2, 3, 4, 5), c(2, 3, 4, 5,))
summ <- data.frame(c(1, 2, 3, 4), c(2, 3, 4, 5), c(2, 3, 4, 5))
summ
?data.frame
summ[2,]
summ[2: ,]
summ[2: ]
summ[2, ]
summ[c(2, 3, 4),]
?kable
summ <- c(summary(greenbuildings$revenue_sqft))
summ_df <- data.frame(summ)
knitr::kable(summ_df, col.names = c("Statistic", "Value"))
?labels
?labs
setwd("~/")
setwd("~/")
setwd("~/")
setwd("~/")
install.packages("portlar")
install.packages("portalr")
?portalr
gc()
source("C:/Users/tinar/dmsl-finalproject/portalproject.R", echo=TRUE)
warnings()
View(working_ts)
View(rodent_data)
library(tidyverse)
library(portalr)
library(lubridate)
library(tidymodels)
library(DiagrammeR) # where is this used?
library(xts)
library(quantmod)
# Load in the data with read_csv
rodent_data <- read_csv("PortalData/Rodents/Portal_rodent.csv")
species_data <- read_csv("PortalData/Rodents/Portal_rodent_species.csv")
### Data cleaning
# Select relevant variables
rodent_data <- rodent_data[, c(1, 2, 4, 8, 9, 31)]
# Replace "" with Unknown
rodent_data[rodent_data == ""] <- "Unknown"
rodent_data <- filter(rodent_data, year >= 1989)
# Let's add more descriptions to rodent data from species data
subset <- species_data[, c(1:4)]
subset <- rename(subset, "species" = "speciescode") #rename variable so they match
rodent_data <- full_join(rodent_data, subset, by = "species")
# Create timestamp for rodent data-- we'll need this for merging monthly counts
for(year in rodent_data["year"]){
for(month in rodent_data["month"]){
rodent_data["timestamp"] = make_datetime(year, month)
}
}
### Feature engineering
# Create species diversity parameter
n_permonth <- rodent_data %>%
group_by(year, month, species) %>%
filter(!is.na(year)) %>%
count() %>%
group_by(year, month) %>%
summarize(permonth = sum(n))
percent_change <- as.data.frame(n_permonth) %>%
mutate(n_permonth = round((permonth/lag(permonth) - 1) * 100, 4))
percent_change[is.na(percent_change)] = 0 # change NA to 0
# Create timestamp for merging
for(year in percent_change["year"]){
for(month in percent_change["month"]){
percent_change["timestamp"] = make_datetime(year, month)
}
}
# Select only time series part with timestamp
percent_change <- subset(percent_change, select = c(n_permonth, timestamp))
#### Time Series Analysis
percent_change$n_permonth = ts(percent_change$n_permonth, deltat = 1/12, start = c(1989, 1, 1))
decm <- decompose(percent_change$n_permonth)
plot(decm)
# Remove seasonality
percent_change$n_permonth <- percent_change$n_permonth - decm$seasonal
acf(percent_change$n_permonth) # shows MA; 1 and 6 lags significant
percent_change$n_permonth <- as.vector(percent_change$n_permonth) # so we can create lags
# Create lagged variable n_permonth
percent_change <- percent_change %>%
mutate(l1_permonth = lag(n_permonth, n = 1, default = 0),
l6_permonth = lag(n_permonth, n = 6, default = 0))
## Let's look at weather data
# The portalr package allows us to summarize weather data monthly
weather_data_raw <- weather(level = "monthly", fill = TRUE, horizon = 30)
# Data cleaning
weather_data <- filter(weather_data_raw, year >= 1989)
# Select relevant columns
weather_data <- weather_data[, c(c(1:6), 9)]
# Create timestamp
for(year in weather_data["year"]){
for(month in weather_data["month"]){
weather_data["timestamp"] = make_datetime(year, month)
}
}
## Create temperature difference variable
weather_data <- weather_data %>%
mutate(tempdiff = maxtemp - mintemp)
### Time series weather data
# Remove seasonality
glimpse(weather_data) # see which variables are time series
weather_ts <- subset(weather_data, select = -c(year, month, timestamp))
for(i in 1:ncol(weather_ts)){
m <- ts(weather_ts[i], deltat = 1/12, start = c(1989, 1, 1))
n <- decompose(m)
j <- m - n$seasonal
weather_ts[i] = as.vector(j)
}
# Create lagged variables of weather time series
weather_ts <- weather_ts %>%
mutate(l_tempdiff = lag(tempdiff, n = 1, default = 0),
l_mintemp = lag(mintemp, n = 1, default = 0),
l_maxtemp = lag(maxtemp, n = 1, default = 0),
l_meantemp = lag(meantemp, n = 1, default = 0),
l_precipitation = lag(precipitation, n = 1, default = 0),
l_warm_days = lag(warm_days, n = 1, default = 0),
l_tempdiff = lag(tempdiff, n = 1, default = 0))
weather_lags <- weather_ts[, c(7:12)]
weather_data <- subset(weather_data, select = -c(year, month))
weather_data <- data.frame(weather_data, weather_lags)
#### Create working data for modeling
# Join percent_change with weather data by timestamp
working_data <- full_join(percent_change, weather_data, by = "timestamp")
rodent_data <- full_join(rodent_data, working_data, by = "timestamp")
## Remove NA
rodent_data <- na.omit(rodent_data)
View(rodent_data)
View(rodent_data)
glimpse(rodent_data)
rodent_num <- subset(rodent_data, select = -c(timestamp, species, recordID, month, year, species, id, scientificname, taxa, commonname))
View(rodent_num)
rodent_num <- rodent_num[unique(stake)]
rodent_num <- rodent_num[unique(rodent_num$stake)]
rodent_num <- rodent_num[unique(rodent_num$stake), ]
View(rodent_num)
rodent_num <- subset(rodent_data, select = -c(timestamp, species, recordID, month, year, species, id, scientificname, taxa, commonname))
working_ts <- subset(rodent_num, select = -c(timestamp))
working_ts <- na.omit(working_ts)
rodents_split <- initial_split(working_ts, prop = 0.8) # 80% of the data will be training the model
rodents_train <- training(rodents_split)
rodents_test <- testing(rodents_split)
# Create folds for cross-validation
folds <- vfold_cv(rodents_train, v = 5)
# First, create the boosting specification with space for tuning
boost_spec <- boost_tree(
trees = 500,
learn_rate = tune(),
tree_depth = tune(),
sample_size = tune()) %>%
set_mode("regression") %>%
set_engine("xgboost")
# Set up tuning grid for tuning hyper-parameters
tunegrid_boost <- grid_regular(parameters(boost_spec), levels = 2)
tune_results <- tune_grid(boost_spec,
n_permonth ~ . ,
resamples = folds,
grid = tunegrid_boost,
metrics = metric_set(rmse))
# Select best parameters
best_boost_params <- select_best(tune_results)
final_boost_spec <- finalize_model(boost_spec, best_boost_params)
boost_model <- final_boost_spec %>% fit(formula = n_permonth ~ .,
data = rodents_train)
## CV in-sample performance
cv_boost <- fit_resamples(final_boost_spec,
n_permonth ~ .,
resamples = folds,
metrics = metric_set(rmse))
collect_metrics(cv_boost)
# Out-of-sample performance
boost_predictions <- boost_model %>%
predict(rodents_test) %>%
bind_cols(rodents_test)
rmse_out_boost <- rmse(boost_predictions,
estimate = .pred,
truth = n_permonth)
rmse_out_boost
### Variable importance
vip::vip(boost_model)
working_ts <- subset(rodent_num)
working_ts <- na.omit(working_ts)
rodents_split <- initial_split(working_ts, prop = 0.8) # 80% of the data will be training the model
rodents_train <- training(rodents_split)
rodents_test <- testing(rodents_split)
View(working_ts)
# Create folds for cross-validation
folds <- vfold_cv(rodents_train, v = 5)
# First, create the boosting specification with space for tuning
boost_spec <- boost_tree(
trees = 500,
learn_rate = tune(),
tree_depth = tune(),
sample_size = tune()) %>%
set_mode("regression") %>%
set_engine("xgboost")
# Set up tuning grid for tuning hyper-parameters
tunegrid_boost <- grid_regular(parameters(boost_spec), levels = 2)
tune_results <- tune_grid(boost_spec,
n_permonth ~ . ,
resamples = folds,
grid = tunegrid_boost,
metrics = metric_set(rmse))
View(percent_change)
best_boost_params <- select_best(tune_results)
final_boost_spec <- finalize_model(boost_spec, best_boost_params)
boost_model <- final_boost_spec %>% fit(formula = n_permonth ~ .,
data = rodents_train)
## CV in-sample performance
cv_boost <- fit_resamples(final_boost_spec,
n_permonth ~ .,
resamples = folds,
metrics = metric_set(rmse))
collect_metrics(cv_boost)
# Out-of-sample performance
boost_predictions <- boost_model %>%
predict(rodents_test) %>%
bind_cols(rodents_test)
rmse_out_boost <- rmse(boost_predictions,
estimate = .pred,
truth = n_permonth)
rmse_out_boost
### Variable importance
vip::vip(boost_model)
?vip
vip::vip(boost_model, aes(col = "yellow"))
vip::vip(boost_model)
vip::vip(boost_model, col = rainbow(10))
?vip
vip::vip(boost_model, aes(col = rainbow(10)))
vip::vip(boost_model, fill = rainbow(10))
vip::vip(boost_model, aesthetics = list(fill = rainbow(10)))
?rainbow
help(package = grDevices)
glimpse(boost_model)
xgb.plot.tree(model = boost_model$finalModel, trees = 1)
class(boost_model)
xgb.plot.tree(model = boost_model, trees = 1)
xgb.plot.tree(model = boost_model$fit, trees = 1)
collect_metrics(cv_boost)
rmse_out_boost
in_metrics <- collect_metrics(cv_boost)
install.packages("htmlTable")
htmlTable::htmlTable(in_metrics$.metric, in_metrics$mean, in_metrics$std_err)
htmlTable::htmlTable(data.frame(c(in_metrics$.metric, in_metrics$mean, in_metrics$std_err)))
colnames(in_table) <- c("Metric", "Cross-Validated Mean Estimate", "Standard Error")
in_table <- data.frame(c(in_metrics$.metric, in_metrics$mean, in_metrics$std_err))
colnames(in_table) <- c("Metric", "Cross-Validated Mean Estimate", "Standard Error")
in_table
best_boost_params <- data.frame(subset(best_boost_params, select = -c(.config)))
knitr::kable(best_boost_params, col.names = c("Tree Depth", "Learn Rate", "Sample Size"))
in_table <- data.frame(subset(in_metrics, select = -c(.config, .estimator)))
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
n_table <- data.frame(subset(in_metrics, select = -c(.config, .estimator)))
in_table <- round(c(in_table$mean, in_table$std_err), 4)
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
in_table[c("mean", "std_err")] <- round(c(in_table$mean, in_table$std_err), 4)
in_table <- data.frame(subset(in_metrics, select = -c(.config, .estimator)))
in_table[c("mean", "std_err")] <- round(c(in_table$mean, in_table$std_err), 4)
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
in_table[c("mean", "std_err")] <- round(c(in_table$mean, in_table$std_err), 8)
in_table[c("mean", "std_err")] <- round(c(in_table$mean, in_table$std_err), 8)
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
in_table[c("mean", "std_err")] <- round(c(in_table$mean, in_table$std_err), 10)
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
in_table[c("mean", "std_err")] <- in_table[c("mean", "std_err") * 1000]
in_table[c("mean", "std_err")] <- in_table[c("mean", "std_err")]*100
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
in_table <- data.frame(subset(in_metrics, select = -c(.config, .estimator)))
in_table[c("mean", "std_err")] <- in_table[c("mean", "std_err")]*100
knitr::kable(in_table, col.names = c("Metric", "Mean Estimate", "Number of folds", "Standard Error"))
rmse_out_boost
out_table <- data.frame(subset(rmse_out_boost, select = -c(.estimator)))
out_table <- data.frame(subset(rmse_out_boost, select = -c(.estimator)))
knitr::kable(out_table, col.names = c("Metric", "Out-of-Sample Estimate"))
View(rodent_data)
rodent_table %>%
group_by(timestamp, species) %>%
summarize(n = n()) %>%
arrange(desc(n))
rodent_data %>%
group_by(timestamp, species) %>%
summarize(n = n()) %>%
arrange(desc(n))
rodent_data %>%
group_by(species, timestamp) %>%
summarize(n = n()) %>%
arrange(desc(n))
rodent_data %>%
group_by(species) %>%
summarize(n = n()) %>%
arrange(desc(n))
head(rodent_data %>%
group_by(species) %>%
summarize(n = n()) %>%
arrange(desc(n)), 5 )#
plot_data <- rodent_data %>%
filter(species = c("PP", "DM", "PB", "DO", "OT"))
plot_data <- rodent_data %>%
filter(species == c("PP", "DM", "PB", "DO", "OT"))
View(plot_data)
plot_data <- rodent_data %>%
filter(species == c("PP", "DM", "PB", "DO", "OT"))
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = "species"))
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species))
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~species)
head(rodent_data %>%
group_by(species) %>%
summarize(n = n()) %>%
arrange(desc(n)), 6) ## Species PP, DM, PB, DO, OT
plot_data <- rodent_data %>%
filter(species == c("PP", "DM", "PB", "DO", "OT", "RM"))
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~species) +
options(repr.plot.width=15, repr.plot.height=8)
?facet_wrap
plot_data <- rodent_data %>%
filter(species == c("PP", "DM", "PB", "DO", "OT"))
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~species, nrow = 5)
top_5 <- head(rodent_data %>%
group_by(scientificname) %>%
summarize(n = n()) %>%
arrange(desc(n)), 5)
top_5
plot_data <- rodent_data %>%
filter(species == top_5$scientificname)
plot_data <- rodent_data %>%
filter(species %in% top_5$scientificname)
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~species, nrow = 5, labeller = c())
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~species, nrow = 5)
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~scientificname, nrow = 5)
###### Data visualization
# What are most popular species?
top_5 <- head(rodent_data %>%
group_by(scientificname) %>%
summarize(n = n()) %>%
arrange(desc(n)), 5)
plot_data <- rodent_data %>%
filter(scientificname %in% top_5$scientificname)
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~scientificname, nrow = 5)
top_5 <- head(rodent_data %>%
group_by(commonname) %>%
summarize(n = n()) %>%
arrange(desc(n)), 5)
plot_data <- rodent_data %>%
filter(commonname %in% top_5$commonname)
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~commonname, nrow = 5)
plot_data <- rodent_data %>%
filter(commonname %in% top_5$commonname | scientificname == "Dipodomys spectabilis")
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~commonname, nrow = 6)
ggplot(plot_data) +
geom_line(aes(x = timestamp, y = n_permonth, col = species)) +
facet_wrap(~commonname, nrow = 6) +
ylab("Percent Change per Month") +
xlab(" ")
?read_csv
setwd("C:/Users/tinar/forecastportal")
ls()
?ls
library(tidyverse)
library(portalr)
library(lubridate)
library(tidymodels)
cat('portalproject.Rmd')
?cat
print('portalproject.Rmd')
setwd("C:/Users/tinar/forecastportal")
setwd("C:/Users/tinar/forecastportal")
rodent_data <- read_csv("Portal_rodent.csv")
species_data <- read_csv("Portal_rodent_species.csv")
# Select relevant variables
rodent_data <- rodent_data[, c(1, 2, 4, 8, 9, 31)]
# Replace "" with Unknown
rodent_data[rodent_data == ""] <- "Unknown"
rodent_data <- filter(rodent_data, year >= 1989)
# Let's add more descriptions to rodent data from species data
subset <- species_data[, c(1:4)]
subset <- rename(subset, "species" = "speciescode") #rename variable so they match
rodent_data <- full_join(rodent_data, subset, by = "species")
# Create timestamp for rodent data-- we'll need this for merging monthly counts
timestamp <- function(data) {
for (year in data["year"]) {
for (month in data["month"]) {
data["timestamp"] <- make_datetime(year, month)
}
}
return(data)
}
rodent_data <- timestamp(rodent_data)
# The portalr package allows us to summarize weather data monthly
weather_data_raw <- weather(level = "monthly",
fill = TRUE,
horizon = 30,
path = "repo")
# Data cleaning
weather_data <- filter(weather_data_raw, year >= 1989)
